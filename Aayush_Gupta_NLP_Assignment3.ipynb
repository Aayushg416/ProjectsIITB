{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFFZMGsxMOHY"
      },
      "source": [
        "# Step 1:Loading Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4rjLXQrMCPj",
        "outputId": "ac80a1ca-1c09-47fa-b55e-3e10179ef718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1A8mqzrqeTUj8Rbh52w0mru_GxONjHVJv\n",
            "To: /content/reviews.csv\n",
            "100%|██████████| 66.2M/66.2M [00:01<00:00, 39.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# Google Drive file ID from the shared link\n",
        "file_id = \"1A8mqzrqeTUj8Rbh52w0mru_GxONjHVJv\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"reviews.csv\", quiet=False)\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "df = pd.read_csv(\"reviews.csv\")\n",
        "print(df.head())\n",
        "\n",
        "#Load & Clean Text\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('reviews.csv')  # Make sure it's in your working directory\n",
        "\n",
        "def clean_text(text): #Removing Extra text and setting everyhting to lowercase\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df['review'] = df['review'].apply(clean_text)\n",
        "\n",
        "#Tokenize & Build Vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "df['tokens'] = df['review'].apply(tokenize)\n",
        "\n",
        "# Build vocab\n",
        "word_counts = Counter([word for tokens in df['tokens'] for word in tokens]) # For Vocab words counting\n",
        "vocab = {word: idx+1 for idx, (word, _) in enumerate(word_counts.items())}  # 0 reserved for padding\n",
        "\n",
        "#Convert to Indexed Sequences & Pad\n",
        "MAX_LEN = 100\n",
        "\n",
        "def encode(tokens):\n",
        "    return [vocab.get(word, 0) for word in tokens][:MAX_LEN]\n",
        "\n",
        "df['encoded'] = df['tokens'].apply(encode)\n",
        "df['padded'] = df['encoded'].apply(lambda x: x + [0]*(MAX_LEN - len(x)))\n",
        "\n",
        "#Train/Val/Test Split\n",
        "X = df['padded'].tolist()\n",
        "y = df['sentiment'].tolist()  # Assuming binary labels: 0 or 1\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) #Dataset ko training, validation, aur test sets me split kar rahe hain\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi2FQwXDMoB0"
      },
      "source": [
        "# **Step 2: Build the LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2isHJMSMpDM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size+1, embed_dim, padding_idx=0) #Word embeddings layer\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True) #LSTM layer for sequence modeling\n",
        "        self.fc = nn.Linear(hidden_dim, 1) #Final classification layer\n",
        "        self.sigmoid = nn.Sigmoid() #Output to probability\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        out = self.fc(hn.squeeze(0))\n",
        "        return self.sigmoid(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueWtQqTsMxoH"
      },
      "source": [
        "# **Step 3: Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkFX7EddMx9Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert string labels to numerical labels\n",
        "label_map = {'positive': 1, 'negative': 0}\n",
        "y_train_numeric = [label_map[label] for label in y_train]\n",
        "y_val_numeric = [label_map[label] for label in y_val]\n",
        "\n",
        "train_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train_numeric))\n",
        "val_data = TensorDataset(torch.tensor(X_val), torch.tensor(y_val_numeric))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64)\n",
        "\n",
        "model = SentimentLSTM(vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.float().to(device)\n",
        "        optimizer.zero_grad() #Previous gradients ko reset\n",
        "        preds = model(xb).squeeze()\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward() #Backpropagation for gradient calculation\n",
        "        optimizer.step() #Model parameters update\n",
        "        epoch_loss += loss.item()\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.float().to(device)\n",
        "            preds = model(xb).squeeze()\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item()\n",
        "        val_losses.append(val_loss / len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kngzvCWbM5xM"
      },
      "source": [
        "# **Step 4: Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuWY4MMrM6Kz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert string labels to numerical labels for the test set\n",
        "label_map = {'positive': 1, 'negative': 0}\n",
        "y_test_numeric = [label_map[label] for label in y_test]\n",
        "\n",
        "test_data = TensorDataset(torch.tensor(X_test), torch.tensor(y_test_numeric))\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = model(xb).squeeze().cpu().numpy()\n",
        "        preds = (preds > 0.5).astype(int)\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(all_labels, all_preds)) #Model ki accuracy\n",
        "print(\"Precision:\", precision_score(all_labels, all_preds)) #Precision metric for positive class\n",
        "print(\"Recall:\", recall_score(all_labels, all_preds)) #Recall metric for positive class\n",
        "print(\"F1 Score:\", f1_score(all_labels, all_preds)) #Harmonic mean of precision and recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0SF-SfoNAwC"
      },
      "source": [
        "# **Step 5: Plot Losses**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOZO7rUONAfU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss') #Training loss vs epochs plot\n",
        "plt.plot(val_losses, label='Val Loss') #Validation loss vs epochs plot\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}